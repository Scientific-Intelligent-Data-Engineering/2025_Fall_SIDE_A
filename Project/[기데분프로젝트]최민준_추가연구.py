# -*- coding: utf-8 -*-
"""[기데분프로젝트]최민준/추가연구

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qN7lcOqUqTfPxKZoUIwLNrpJIj5LGKgM
"""

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout

import pickle

# 1) 댓글 + 기존 감성 라벨이 들어있는 파일 불러오기
df = pd.read_csv("youtube_comments_2024_sentiment.csv")

# 2) 결측치 제거
df = df.dropna(subset=["comment_text", "sentiment_label"]).copy()

print("데이터 개수:", len(df))
print(df[["comment_text", "sentiment_label"]].head())

# 1) sentiment_label → 숫자 라벨로 변환
label_encoder = LabelEncoder()
df["label_id"] = label_encoder.fit_transform(df["sentiment_label"])

print("라벨 인코딩 결과:")
for idx, name in enumerate(label_encoder.classes_):
    print(f"{idx} -> {name}")

num_classes = len(label_encoder.classes_)

# 2) train / valid / test 나누기 (stratify로 비율 유지)
train_df, test_df = train_test_split(
    df,
    test_size=0.15,
    random_state=2025170924,
    stratify=df["label_id"],
)

train_df, val_df = train_test_split(
    train_df,
    test_size=0.15,
    random_state=2025170924,
    stratify=train_df["label_id"],
)

print("train:", len(train_df), " / val:", len(val_df), " / test:", len(test_df))

# 3) Keras Tokenizer를 이용해서 텍스트 → 정수 시퀀스로 변환

MAX_WORDS = 100_000   # 단어 사전 크기(상위 50k 단어만 사용)
MAX_LEN   = 50       # 한 댓글당 최대 토큰 길이

tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token="<UNK>")
tokenizer.fit_on_texts(train_df["comment_text"])

def texts_to_padded(texts, maxlen=MAX_LEN):
    seqs = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(
        seqs,
        maxlen=maxlen,
        padding="post",
        truncating="post"
    )
    return padded

X_train = texts_to_padded(train_df["comment_text"])
X_val   = texts_to_padded(val_df["comment_text"])
X_test  = texts_to_padded(test_df["comment_text"])

y_train = train_df["label_id"].values
y_val   = val_df["label_id"].values
y_test  = test_df["label_id"].values

X_train.shape, X_val.shape, X_test.shape

# 1) 클래스 가중치 계산 (neutral이 훨씬 많아서 균형 맞추기)
class_weights_arr = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(y_train),
    y=y_train
)
class_weights = {i: w for i, w in enumerate(class_weights_arr)}
print("class_weights:", class_weights)

# 2) LSTM 기반 감성 분석 모델 정의

EMBED_DIM  = 128
LSTM_UNITS = 128

model = Sequential([
    Embedding(
        input_dim=MAX_WORDS,
        output_dim=EMBED_DIM,
        input_length=MAX_LEN
    ),
    Bidirectional(LSTM(LSTM_UNITS, return_sequences=False)),
    Dropout(0.5),
    Dense(64, activation="relu"),
    Dropout(0.5),
    Dense(num_classes, activation="softmax")
])

model.compile(
    loss="sparse_categorical_crossentropy",
    optimizer="adam",
    metrics=["accuracy"]
)

model.summary()

BATCH_SIZE = 512   # 메모리 부족하면 256, 128로 줄이기
EPOCHS     = 2     # 시간이 너무 오래 걸리면 3 정도로 줄이기

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    class_weight=class_weights,
    verbose=1
)

test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)
print(f"\n[테스트 성능] loss={test_loss:.4f}, acc={test_acc:.4f}")

# 확률 → 예측 라벨
y_pred_prob = model.predict(X_test, batch_size=BATCH_SIZE)
y_pred = np.argmax(y_pred_prob, axis=1)

print("\n[분류 리포트]")
print(classification_report(
    y_test,
    y_pred,
    target_names=label_encoder.classes_
))

import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(5,4))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    xticklabels=label_encoder.classes_,
    yticklabels=label_encoder.classes_
)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Sentiment Confusion Matrix (LSTM)")
plt.show()

# 1) 모델 저장
model.save("sentiment_lstm_model1.h5")

# 2) 토크나이저 저장
with open("sentiment_tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)

# 3) 라벨 인코더도 같이 저장해두면 좋음
with open("sentiment_label_encoder.pkl", "wb") as f:
    pickle.dump(label_encoder, f)

# 새 댓글의 감성을 예측하는 함수
def predict_sentiment(texts, top_k=1):
    """
    texts: 문자열 또는 문자열 리스트
    top_k: 상위 k개 클래스와 확률까지 보고 싶을 때 (지금은 1만 사용해도 됨)
    """
    if isinstance(texts, str):
        texts = [texts]

    seq = texts_to_padded(texts)
    probs = model.predict(seq)
    label_ids = np.argmax(probs, axis=1)
    labels = label_encoder.inverse_transform(label_ids)
    confidences = probs.max(axis=1)

    results = []
    for t, lab, conf, p in zip(texts, labels, confidences, probs):
        info = {
            "text": t,
            "pred_label": lab,
            "confidence": float(conf),
        }
        if top_k > 1:
            top_indices = np.argsort(p)[::-1][:top_k]
            info["top_k"] = [
                (label_encoder.classes_[i], float(p[i])) for i in top_indices
            ]
        results.append(info)
    return results

from tensorflow.keras.models import load_model
import pickle
import numpy as np
import pandas as pd

# 학습해 둔 모델/토크나이저/라벨 인코더 로드
model = load_model("sentiment_lstm_model1.h5")

with open("sentiment_tokenizer.pkl", "rb") as f:
    tokenizer = pickle.load(f)

with open("sentiment_label_encoder.pkl", "rb") as f:
    label_encoder = pickle.load(f)

MAX_LEN = 50  # 학습 때 썼던 값과 똑같이

from tensorflow.keras.preprocessing.sequence import pad_sequences

def texts_to_padded(texts, maxlen=MAX_LEN):
    seqs = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(
        seqs,
        maxlen=maxlen,
        padding="post",
        truncating="post"
    )
    return padded

# 혹시 라벨이 어떻게 나왔는지 확인
print(np.unique(comments_df["sentiment_ai"]))

# 1) 영상별 댓글/선플/악플 개수 집계
negative_name = "negative"  # 실제 라벨명이 "neg"면 "neg"로 바꿔줘
positive_name = "positive"

agg_ai = comments_df.groupby("video_id").agg(
    n_comments_ai=("comment_text", "size"),
    n_negative_ai=("sentiment_ai", lambda x: (x == negative_name).sum()),
    n_positive_ai=("sentiment_ai", lambda x: (x == positive_name).sum()),
)

agg_ai["ratio_negative_ai"] = agg_ai["n_negative_ai"] / agg_ai["n_comments_ai"]
agg_ai["ratio_positive_ai"] = agg_ai["n_positive_ai"] / agg_ai["n_comments_ai"]

agg_ai.head()

video_df = pd.read_csv("youtube_2024_video_comment_merged.csv")

# video_df 안에도 video_id가 있어야 함 (없으면 컬럼명 맞추거나 merge 기준만 바꿔줘)
merged_ai = video_df.merge(agg_ai, on="video_id", how="left")

# 댓글 없는 영상은 악플 비율 계산 불가 → 필요하다면 제거
merged_ai = merged_ai.dropna(subset=["n_comments_ai"]).copy()

print(len(merged_ai))
merged_ai[["video_id", "view_count", "n_comments_ai", "ratio_negative_ai"]].head()

# 댓글:조회수 비율 (AI 기준)
merged_ai["comment_view_ratio_ai"] = merged_ai["n_comments_ai"] / merged_ai["view_count"]

# 로그 조회수
merged_ai["log_view_count"] = np.log10(merged_ai["view_count"])

cols = ["ratio_negative_ai", "comment_view_ratio_ai", "log_view_count"]
corr_matrix = merged_ai[cols].corr()

print(corr_matrix)
print("\n악플 비율(AI) vs 로그 조회수 상관계수:",
      corr_matrix.loc["ratio_negative_ai", "log_view_count"])
print("악플 비율(AI) vs 댓글:조회수 비율 상관계수:",
      corr_matrix.loc["ratio_negative_ai", "comment_view_ratio_ai"])

min_comments = 10
filtered = merged_ai.query("n_comments_ai >= @min_comments").copy()

corr_filtered = filtered[cols].corr()
print("\n[댓글 10개 이상 영상만]")
print(corr_filtered)

!apt-get -y install fonts-nanum

import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import matplotlib as mpl

# 1) 나눔고딕 폰트 파일 경로 (fc-list 결과에 맞게 필요하면 파일명만 바꿔)
font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"

# 2) 이 폰트를 matplotlib에 등록
fm.fontManager.addfont(font_path)
font_prop = fm.FontProperties(fname=font_path)

# 3) 전역 기본 폰트로 설정
mpl.rcParams['font.family'] = font_prop.get_name()
mpl.rcParams['axes.unicode_minus'] = False

print("설정된 폰트 이름:", font_prop.get_name())

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sc = plt.scatter(
    merged_ai["ratio_negative_ai"],
    merged_ai["comment_view_ratio_ai"],
    c=merged_ai["log_view_count"],
    alpha=0.5
)

plt.colorbar(sc, label="log10(조회수)")
plt.xlabel("악플 비율 (AI, ratio_negative_ai)")
plt.ylabel("댓글:조회수 비율 (AI, comment_view_ratio_ai)")
plt.title("AI 악플 비율 vs 댓글 활성도 (색: 조회수 규모, 로그)")
plt.tight_layout()
plt.show()

if "ratio_negative" in merged_ai.columns:
    print("규칙 기반 악플 비율 vs 로그 조회수:",
          merged_ai[["ratio_negative", "log_view_count"]].corr().iloc[0,1])

print("AI 악플 비율 vs 로그 조회수:",
      merged_ai[["ratio_negative_ai", "log_view_count"]].corr().iloc[0,1])

import pandas as pd
import numpy as np
import pickle
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt

# ====== 경로 설정 ======
VIDEO_CSV_PATH   = "/content/youtube_2024_video_comment_merged.csv"   # 영상 단위 데이터
COMMENTS_CSV_PATH = "/content/youtube_comments_2024_sentiment.csv"    # 댓글 단위 데이터 (sentiment_... 그 파일)

# ====== 1) 영상 단위 데이터 불러오기 ======
df = pd.read_csv(VIDEO_CSV_PATH)

# 영상 중 조회수 0인 건 로그에서 문제되니까 제거 (원래 ipynb에서도 비슷하게 했을 거야)
df = df[df["view_count"] > 0].copy()

# 로그 조회수 컬럼 만들기
df["log_view_count"] = np.log10(df["view_count"])

print("영상 데이터 shape:", df.shape)
print(df.head(3))

# ====== 2) 댓글 단위 데이터 불러오기 ======
comments_df = pd.read_csv(COMMENTS_CSV_PATH)
comments_df = comments_df.dropna(subset=["comment_text"]).copy()

print("댓글 데이터 shape:", comments_df.shape)
print(comments_df[["video_id", "comment_text"]].head(3))

# ====== 1) 모델 / 토크나이저 / 라벨 인코더 불러오기 ======
model = load_model("/content/sentiment_lstm_model1.h5", compile=False)

with open("/content/sentiment_tokenizer.pkl", "rb") as f:
    tokenizer = pickle.load(f)

with open("/content/sentiment_label_encoder.pkl", "rb") as f:
    label_encoder = pickle.load(f)

MAX_LEN = 50  # 학습할 때 썼던 값 그대로


def texts_to_padded(texts, maxlen=MAX_LEN):
    seqs = tokenizer.texts_to_sequences(texts.astype(str))
    return pad_sequences(seqs, maxlen=maxlen, padding="post", truncating="post")


# ====== 2) 전체 댓글에 대해 AI 감성 예측 ======
X_all = texts_to_padded(comments_df["comment_text"])
pred_probs = model.predict(X_all, batch_size=512, verbose=1)

pred_ids = np.argmax(pred_probs, axis=1)
pred_labels = label_encoder.inverse_transform(pred_ids)

comments_df["sentiment_ai"] = pred_labels

print("AI 감성 라벨 종류:", np.unique(comments_df["sentiment_ai"]))
comments_df[["video_id", "comment_text", "sentiment_ai"]].head()

# 라벨 이름 확인해서 맞게 넣어줘
negative_name = "negative"
positive_name = "positive"

# ====== 1) 영상별 댓글/선플/악플 개수 집계 ======
agg_ai = comments_df.groupby("video_id").agg(
    n_comments_ai=("comment_text", "size"),
    n_negative_ai=("sentiment_ai", lambda x: (x == negative_name).sum()),
    n_positive_ai=("sentiment_ai", lambda x: (x == positive_name).sum()),
)

agg_ai["ratio_negative_ai"] = agg_ai["n_negative_ai"] / agg_ai["n_comments_ai"]
agg_ai["ratio_positive_ai"] = agg_ai["n_positive_ai"] / agg_ai["n_comments_ai"]

print(agg_ai.head())

# ====== 2) 원래 영상 df에 merge ======
df = df.merge(agg_ai, on="video_id", how="left")

# 댓글 없는 영상은 분석에서 제외 (원하면 조건 숫자 조절)
df = df.dropna(subset=["n_comments_ai"]).copy()
df = df[df["n_comments_ai"] >= 5].copy()  # 댓글 최소 5개 이상인 영상만 사용 (선택)

print("merge 후 df shape:", df.shape)
df[["video_id", "view_count", "n_comments_ai",
    "ratio_negative_ai", "ratio_positive_ai"]].head()

cols = ["view_count", "log_view_count",
        "ratio_negative_ai", "ratio_positive_ai"]

corr_all = df[cols].corr()
print("=== 전체 영상 상관계수 ===")
print(corr_all)

print("\n악플 비율 vs 로그 조회수:",
      corr_all.loc["ratio_negative_ai", "log_view_count"])
print("선플 비율 vs 로그 조회수:",
      corr_all.loc["ratio_positive_ai", "log_view_count"])

!apt-get -y install fonts-nanum

import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import matplotlib as mpl

# 1) 나눔고딕 폰트 파일 경로 (fc-list 결과에 맞게 필요하면 파일명만 바꿔)
font_path = "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"

# 2) 이 폰트를 matplotlib에 등록
fm.fontManager.addfont(font_path)
font_prop = fm.FontProperties(fname=font_path)

# 3) 전역 기본 폰트로 설정
mpl.rcParams['font.family'] = font_prop.get_name()
mpl.rcParams['axes.unicode_minus'] = False

print("설정된 폰트 이름:", font_prop.get_name())

plt.figure(figsize=(7, 5))
plt.scatter(df["ratio_negative_ai"], df["log_view_count"], alpha=0.4)
plt.xlabel("악플 비율 (AI, ratio_negative_ai)")
plt.ylabel("log10(조회수)")
plt.title("전체 영상: 악플 비율 vs 로그 조회수")
plt.tight_layout()
plt.show()

plt.figure(figsize=(7, 5))
plt.scatter(df["ratio_positive_ai"], df["log_view_count"], alpha=0.4)
plt.xlabel("선플 비율 (AI, ratio_positive_ai)")
plt.ylabel("log10(조회수)")
plt.title("전체 영상: 선플 비율 vs 로그 조회수")
plt.tight_layout()
plt.show()

# 카테고리별로 요약 통계
cat_summary = (
    df.groupby("category_name")
      .agg(
          n_videos=("video_id", "nunique"),          # 카테고리별 영상 수
          mean_view=("view_count", "mean"),          # 평균 조회수
          median_view=("view_count", "median"),      # 중앙값 조회수
          mean_log_view=("log_view_count", "mean"),  # 평균 log10(조회수)
          mean_ratio_neg=("ratio_negative_ai", "mean"),  # 평균 악플 비율
          mean_ratio_pos=("ratio_positive_ai", "mean"),  # 평균 선플 비율
      )
      .reset_index()
)

cat_summary.sort_values("mean_view", ascending=False, inplace=True)
cat_summary

plt.figure(figsize=(10, 5))
plt.bar(cat_summary["category_name"], cat_summary["mean_ratio_neg"])
plt.xticks(rotation=45, ha="right")
plt.ylabel("평균 악플 비율 (AI)")
plt.title("카테고리별 평균 악플 비율")
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
plt.bar(cat_summary["category_name"], cat_summary["mean_ratio_pos"])
plt.xticks(rotation=45, ha="right")
plt.ylabel("평균 선플 비율 (AI)")
plt.title("카테고리별 평균 선플 비율")
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np

# 필요한 컬럼이 비어있는 행 제거
df_cat = df.dropna(subset=[
    "category_name",
    "ratio_negative_ai",
    "ratio_positive_ai",
    "log_view_count"
]).copy()


def corr_by_category(group):
    return pd.Series({
        "n_videos": len(group),
        "corr_neg_logview": group["ratio_negative_ai"].corr(group["log_view_count"]),
        "corr_pos_logview": group["ratio_positive_ai"].corr(group["log_view_count"]),
        # 필요하면 원조회수 기준 상관계수도
        "corr_neg_view": group["ratio_negative_ai"].corr(group["view_count"]),
        "corr_pos_view": group["ratio_positive_ai"].corr(group["view_count"]),
    })

corr_cat = (
    df_cat
    .groupby("category_name")
    .apply(corr_by_category)
    .reset_index()
)


# 1) 영상 10개 이상 있는 카테고리만 필터링
corr_cat_10 = corr_cat[corr_cat["n_videos"] >= 10].copy()

# 2) 보기 좋게 정렬 (예: 악플-조회수 상관계수 기준으로)
corr_cat_10 = corr_cat_10.sort_values("corr_neg_logview", ascending=False)

corr_cat_10